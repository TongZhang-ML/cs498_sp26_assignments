\begin{Q}
\textbf{Text Generation: GPT-2 Fine-tuning vs.\ Training from Scratch. 30 Points}

In this problem, you will compare two approaches for text generation (causal language modeling) on a small text corpus:
(i) fine-tune a pretrained GPT-2 model, and (ii) train a small Transformer language model from scratch.
Both approaches will be trained and evaluated on the same dataset split, and you will report their training and validation performance.

The dataset contains  
\[
\texttt{./data/train.txt}, \quad \texttt{./data/val.txt}.
\]

You are provided with a template Python file for this problem.
Your task is to fill in \emph{only} the functions whose docstring begins with \textbf{Implement:}.
For clarity, the required functions in this assignment are:
\[
\texttt{build\_scratch\_model},\ 
\texttt{train\_scratch\_model}.
\]

\medskip
\noindent\textbf{Data format.}
The files \texttt{train.txt} and \texttt{val.txt} are line separated plain text files.
The provided template code will:
\begin{itemize}
\item Tokenize each line using the GPT-2 tokenizer.
\item Concatenate tokens into a single stream (with EOS tokens inserted between lines).
\item Split the stream into fixed-length sequences of length \texttt{context\_length}.
\item Pad the final sequence and use an \texttt{attention\_mask} so padded positions do not affect training.
\end{itemize}
You do not need to modify any dataset-related code.

\medskip
\noindent\textbf{Models.}
\begin{itemize}
\item \textbf{Fine-tuning:} The template already fine-tunes
\texttt{GPT2LMHeadModel.from\_pretrained("gpt2")}.
You do not need to modify this part.

\item \textbf{From scratch:} You will construct a small causal Transformer language model using a provided configuration class \texttt{ScratchConfig} and a provided model class \texttt{CustomGPT2LM}.
\end{itemize}

\medskip
\noindent\textbf{Training protocol.}
The template uses Hugging Face \\
\texttt{Trainer} and 
\texttt{DataCollatorForLanguageModeling(mlm=False)}.
Your implementation of the scratch model should follow the same training pipeline.

\medskip
\noindent\textbf{Prompt for generation.}
Both models will generate text using the fixed prompt:
\[
\texttt{"A fox sits"}
\]

\begin{enumerate}

\item \textbf{\texttt{build\_scratch\_model} (10 points).}

Implement:
\[
\texttt{build\_scratch\_model}
\]
according to its docstring.

\item \textbf{\texttt{train\_scratch\_model} (10 points).}

Implement:
\[
\texttt{train\_scratch\_model}
\]
according to its docstring.

\item \textbf{Comparison experiment and report (10 points).}

Run your solution, 
which will print a LaTeX snippet.
Copy and paste the generated LaTeX into your solution.

Your report should include:
\begin{itemize}
\item A table with \textbf{training loss and validation loss} for:
  \begin{itemize}
  \item GPT-2 fine-tune for 1,3,5 epochs
  \item Scratch Transformer for 5,10,15 epochs
  \end{itemize}
\item The generated text from the models using the prompt \texttt{"A
    fox sits"} for corresponding epochs.
\item Discuss your observations for finetuning and training from scratch.
\end{itemize}

\begin{solution}


\noindent \textbf{GPT-2 fine-tune (different epochs).}
\begin{tabular}{lcc}
\hline
Epochs & Train loss & Val loss \\
\hline
1 & 3.7138 & 3.8588 \\
3 & 3.1873 & 3.7630 \\
5 & 2.8231 & 3.8060 \\
\hline
\end{tabular}

\noindent \textbf{Scratch Transformer (different epochs).}
\begin{tabular}{lcc}
\hline
Epochs & Train loss & Val loss \\
\hline
5 & 5.3791 & 6.1658 \\
10 & 4.4266 & 5.9560 \\
15 & 3.5957 & 6.0848 \\
\hline
\end{tabular}

\medskip
\noindent Prompt: A fox sits\\

\noindent \textbf{Generated samples (GPT-2 fine-tune).}\\
\noindent Epochs 1: A fox sits on the edge of a pond, thinking what to do next. He wants to be on the water and see if it will be okay for him. He sticks his tongue out and soently caresses the water, the Fox is afraid of him, and his wings flutter high up.”\\
\noindent Epochs 3: A fox sits down on one of the benches in front of him, and supposing him to be an animal of great size, he smites the bird, and the Fox looks about him to see whether it is a Cock or a Cockney Eagle. Æsop, the most learned and high-pri\\
\noindent Epochs 5: A fox sits on the hillside and quietly ponders the contents of his nets, while the Man ponders the fish with his sad eyes. At last, being about to attempt to catch a fish, he produces a great roar and smites the Fox with his teeth. The Fox, frightened, thus resigns\\

\noindent \textbf{Generated samples (Scratch Transformer).}\\
\noindent Epochs 5: A fox sits quiet Cock, and theri each thus BO” The preferable into the Malt the Sun of inquired to his theil those E, he was the light that the Hare, but up and the tree his contempt child a one by she onumb place and named notself to all, made one this\\
\noindent Epochs 10: A fox sits, away days, while a new under a Wolf and the Sheep was and passengers by one of your replied, and known of the Fox and to the movements. Heifer me. The Sea lad his hands to have beenia by those in their apart the Ass, being, buzzing. The Fox,\\
\noindent Epochs 15: A fox sits noted become for his throat.” “But him, endeavoring him, and I am across is not able to draw is known, when the bank to complainry, and Hence to come with his poet. Loud you will be a King. When the Holy body it and her to the\\
  

  
  Observation. 
  Fineuting achieves better loss and more fluent generation. However,
  both starts to overfit (validation loss starts to increase) when
  trained for too many epochs. 
\end{solution}

\end{enumerate}
\end{Q}