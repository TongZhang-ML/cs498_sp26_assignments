\begin{Q}
\textbf{Text Generation: GPT-2 Fine-tuning vs.\ Training from Scratch. 30 Points}

In this problem, you will compare two approaches for text generation (causal language modeling) on a small text corpus:
(i) fine-tune a pretrained GPT-2 model, and (ii) train a small Transformer language model from scratch.
Both approaches will be trained and evaluated on the same dataset split, and you will report their training and validation performance.

The dataset contains  
\[
\texttt{./data/train.txt}, \quad \texttt{./data/val.txt}.
\]

You are provided with a template Python file for this problem.
Your task is to fill in \emph{only} the functions whose docstring begins with \textbf{Implement:}.
For clarity, the required functions in this assignment are:
\[
\texttt{build\_scratch\_model},\ 
\texttt{train\_scratch\_model}.
\]

\medskip
\noindent\textbf{Data format.}
The files \texttt{train.txt} and \texttt{val.txt} are line separated plain text files.
The provided template code will:
\begin{itemize}
\item Tokenize each line using the GPT-2 tokenizer.
\item Concatenate tokens into a single stream.
\item Split the stream into fixed-length sequences of length \texttt{context\_length}.
\item Pad the final sequence and use an \texttt{attention\_mask} so padded positions do not affect training.
\end{itemize}
You do not need to modify any dataset-related code.

\medskip
\noindent\textbf{Models.}
\begin{itemize}
\item \textbf{Fine-tuning:} The template already fine-tunes
\texttt{GPT2LMHeadModel.from\_pretrained("gpt2")}.
You do not need to modify this part.

\item \textbf{From scratch:} You will construct a small causal Transformer language model using a provided configuration class \texttt{ScratchConfig} and a provided model class \texttt{CustomGPT2LM}.
\end{itemize}

\medskip
\noindent\textbf{Training protocol.}
The template uses Hugging Face \texttt{Trainer} for training and evaluation.
For causal language modeling, padded positions are masked in the loss by setting their labels to \texttt{-100}, so that they do not contribute to the training objective.
Your implementation of the scratch model should follow the same training pipeline.

\medskip
\noindent\textbf{Prompt for generation.}
Both models will generate text using the fixed prompt:
\[
\texttt{"A fox sits"}
\]

\begin{enumerate}

\item \textbf{\texttt{build\_scratch\_model} (10 points).}

Implement:
\[
\texttt{build\_scratch\_model}
\]
according to its docstring.

\item \textbf{\texttt{train\_scratch\_model} (10 points).}

Implement:
\[
\texttt{train\_scratch\_model}
\]
according to its docstring.

\item \textbf{Comparison experiment and report (10 points).}

Run your solution, 
which will print a LaTeX snippet.
Copy and paste the generated LaTeX into your solution.

Your report should include:
\begin{itemize}
\item A table with \textbf{training loss and validation loss} for:
  \begin{itemize}
  \item GPT-2 fine-tune for 1,3,5 epochs
  \item Scratch Transformer for 5,10,15 epochs
  \end{itemize}
\item The generated text from the models using the prompt \texttt{"A fox sits"} for corresponding epochs.
\item Discuss your observations for finetuning and training from scratch.
\end{itemize}

\begin{solution}

\noindent \textbf{GPT-2 fine-tune (different epochs).}
\begin{tabular}{lcc}
\hline
Epochs & Train loss & Val loss \\
\hline
1 & 3.1929 & 3.3849 \\
3 & 2.6483 & 3.2493 \\
5 & 2.2771 & 3.2836 \\
\hline
\end{tabular}

\noindent \textbf{Scratch Transformer (different epochs).}
\begin{tabular}{lcc}
\hline
Epochs & Train loss & Val loss \\
\hline
5 & 5.2260 & 6.0942 \\
10 & 4.2456 & 5.7645 \\
15 & 3.4487 & 5.8662 \\
\hline
\end{tabular}

\medskip
\noindent Prompt: A fox sits\\

\noindent \textbf{Generated samples (GPT-2 fine-tune).}\\
\noindent Epochs 1: A fox sits in the company of a goat, and he is not content with his own rights, but his own life. He leaves his cubs, and drowns into the water, which are all for him. The Goat, seeing these circumstances, pours a\\
\noindent Epochs 3: A fox sits on a bridge, and perches on his neck. An Ox, on seeing him, perceiving him, turns round to see if he might find some honey. The Horse, feeling a sudden burst of pain, cries out, “O! my little fellow! I see\\
\noindent Epochs 5: A fox sits at the gate of his house, looking at the wheat-seller. A Lion comes in and brings with him some hay. Standing before him a bundle of gold, and a chain of jewels, which he possess in the course of his journey. The Lion inquired of\\

\noindent \textbf{Generated samples (Scratch Transformer).}\\
\noindent Epochs 5: A fox sits. On that he was in his his of the Ox, and the Lion into a was to his have, “My.”\\
\noindent Epochs 10: A fox sits up, and not gone a per him and when the young, and the Fox to these, will and the same the Housed, and the ex great fables to be drink as, but to his life. The Ass, he said, “Sir for the the Ass\\
\noindent Epochs 15: A fox sits-lime, seeing a great young, and a great clad to him. for his death, the Lion, that he went he said: “Yes, but this for a pole, as I am all his you.” he returned able to us to me the most me\\

\vspace*{1cm}
\noindent \textbf{Observation.} 
Fine-tuning achieves better loss and more fluent generation. However,
both start to overfit (validation loss begins to increase) when
trained for too many epochs. 
\end{solution}

\end{enumerate}
\end{Q}